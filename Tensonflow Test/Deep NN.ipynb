{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a network with -\n",
    "2 Hidden layers and one output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    #initialising the network to have a random linear functions. Generating a whole NN.\n",
    "    def __init__(self,D_in,H1,H2,D_out):\n",
    "        super(Net,self).__init__()\n",
    "        #now i have created a child class of Module. \n",
    "        #I can call functions of parent class \n",
    "        #and store it as objects of child class\n",
    "        self.Linear1=nn.Linear(D_in,H1)\n",
    "        self.Linear2=nn.Linear(H1,H2)\n",
    "        #the represents the output layer\n",
    "        self.Linear3=nn.Linear(H2,D_in)\n",
    "    def forward(self,x):\n",
    "        x=torch.sigmoid(self.Linear1(x))\n",
    "        x=torch.sigmoid(self.Linear2(x))\n",
    "        x=self.Linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**  \n",
    "- If you want to implement examples in one node. You can put every example in each row of a tensor torch\n",
    "- And if you want to put different features. Its always better to put them in different nodes all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:28, 336476.89it/s]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A\n",
      " 57%|█████▋    | 16384/28881 [00:00<00:00, 58095.00it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 16384/1648877 [00:00<00:29, 55447.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 40960/1648877 [00:01<00:25, 62427.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 98304/1648877 [00:01<00:19, 78453.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 180224/1648877 [00:01<00:14, 99594.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 253952/1648877 [00:02<00:11, 123962.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 294912/1648877 [00:02<00:08, 154608.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 344064/1648877 [00:02<00:07, 171063.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 409600/1648877 [00:02<00:05, 216354.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 450560/1648877 [00:02<00:05, 202872.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 524288/1648877 [00:02<00:04, 227888.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 565248/1648877 [00:03<00:04, 256015.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 598016/1648877 [00:03<00:05, 176530.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 704512/1648877 [00:03<00:04, 216725.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 745472/1648877 [00:03<00:03, 243134.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 778240/1648877 [00:03<00:03, 229088.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 819200/1648877 [00:03<00:03, 253732.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 860160/1648877 [00:04<00:03, 236965.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 909312/1648877 [00:04<00:02, 271467.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 942080/1648877 [00:04<00:02, 258435.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 983040/1648877 [00:04<00:02, 289424.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 1032192/1648877 [00:04<00:02, 274642.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1081344/1648877 [00:04<00:01, 303317.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 1122304/1648877 [00:05<00:01, 281737.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 1171456/1648877 [00:05<00:01, 315300.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 1212416/1648877 [00:05<00:01, 288101.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 1269760/1648877 [00:05<00:01, 323088.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1310720/1648877 [00:05<00:01, 298422.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1359872/1648877 [00:05<00:00, 337723.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 1400832/1648877 [00:05<00:00, 332690.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1441792/1648877 [00:06<00:00, 309051.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 1482752/1648877 [00:06<00:00, 313826.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 1523712/1648877 [00:06<00:00, 334935.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 1572864/1648877 [00:06<00:00, 352216.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 1613824/1648877 [00:06<00:00, 308468.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "1654784it [00:06, 250835.60it/s]                             \u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "8192it [00:00, 13451.88it/s]            \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:40, 336476.89it/s]\n",
      "32768it [00:20, 58095.00it/s]                           \u001b[A"
     ]
    }
   ],
   "source": [
    "#this has popular training datasets\n",
    "train_dataset=dset.MNIST(root='./data',train=True,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting a loss function\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model class using Relu as a activation function\n",
    "\n",
    "class NetRelu(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(NetRelu, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  \n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Trainloader lets you extract the data in x and y\n",
    "\n",
    "def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_accuracy': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.data.item())\n",
    "        \n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            yhat = model(x.view(-1, 28 * 28))\n",
    "            _, label = torch.max(yhat, 1)\n",
    "            correct += (label == y).sum().item()\n",
    "    \n",
    "        accuracy = 100 * (correct / len(validation_dataset))\n",
    "        useful_stuff['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return useful_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validating dataset\n",
    "\n",
    "validation_dataset = dset.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for create the model\n",
    "# image is of dimension 28*28. It feeds in the network. Output is of a digit from 1 to 10.\n",
    "\n",
    "input_dim = 28 * 28\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 = 50\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of iterations\n",
    "\n",
    "cust_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data loader and validation data loader object\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x123f249b0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jist of optimisers](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with sigmoid function\n",
    "\n",
    "learning_rate = 0.01\n",
    "model = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_loss': [6.711361408233643,\n",
       "  6.719888210296631,\n",
       "  6.698922634124756,\n",
       "  6.669858932495117,\n",
       "  6.659979343414307,\n",
       "  6.645028591156006,\n",
       "  6.636767387390137,\n",
       "  6.615419387817383,\n",
       "  6.600926399230957,\n",
       "  6.591947555541992,\n",
       "  6.576782703399658,\n",
       "  6.5601911544799805,\n",
       "  6.550868988037109,\n",
       "  6.541611194610596,\n",
       "  6.51452112197876,\n",
       "  6.503509998321533,\n",
       "  6.4915852546691895,\n",
       "  6.477435111999512,\n",
       "  6.451359748840332,\n",
       "  6.446980953216553,\n",
       "  6.435943126678467,\n",
       "  6.4193315505981445,\n",
       "  6.408897876739502,\n",
       "  6.396498680114746,\n",
       "  6.3775835037231445,\n",
       "  6.365340232849121,\n",
       "  6.365054607391357,\n",
       "  6.3460235595703125,\n",
       "  6.342011451721191,\n",
       "  6.307867050170898,\n",
       "  6.309206485748291,\n",
       "  6.286592960357666,\n",
       "  6.266880035400391,\n",
       "  6.256866931915283,\n",
       "  6.245728969573975,\n",
       "  6.225630283355713,\n",
       "  6.220235824584961,\n",
       "  6.200465202331543,\n",
       "  6.191842079162598,\n",
       "  6.167851448059082,\n",
       "  6.161558151245117,\n",
       "  6.13621187210083,\n",
       "  6.129256248474121,\n",
       "  6.121739387512207,\n",
       "  6.103392124176025,\n",
       "  6.102561950683594,\n",
       "  6.093287944793701,\n",
       "  6.0539751052856445,\n",
       "  6.054338455200195,\n",
       "  6.042722225189209,\n",
       "  6.028597831726074,\n",
       "  6.004675388336182,\n",
       "  5.983216285705566,\n",
       "  5.974066257476807,\n",
       "  5.964401721954346,\n",
       "  5.967133522033691,\n",
       "  5.941225051879883,\n",
       "  5.924499988555908,\n",
       "  5.906651020050049,\n",
       "  5.894410133361816,\n",
       "  5.8848185539245605,\n",
       "  5.863314628601074,\n",
       "  5.862286567687988,\n",
       "  5.840938568115234,\n",
       "  5.822440147399902,\n",
       "  5.814496994018555,\n",
       "  5.80290412902832,\n",
       "  5.788042068481445,\n",
       "  5.7645039558410645,\n",
       "  5.757768154144287,\n",
       "  5.736393451690674,\n",
       "  5.7198805809021,\n",
       "  5.713881492614746,\n",
       "  5.704355239868164,\n",
       "  5.684638977050781,\n",
       "  5.666881084442139,\n",
       "  5.665887832641602,\n",
       "  5.624917030334473,\n",
       "  5.631167411804199,\n",
       "  5.61491060256958,\n",
       "  5.599852561950684,\n",
       "  5.5840373039245605,\n",
       "  5.580875873565674,\n",
       "  5.56870174407959,\n",
       "  5.551189422607422,\n",
       "  5.527261257171631,\n",
       "  5.505195617675781,\n",
       "  5.487818241119385,\n",
       "  5.497312545776367,\n",
       "  5.4787516593933105,\n",
       "  5.454470634460449,\n",
       "  5.445361137390137,\n",
       "  5.4395060539245605,\n",
       "  5.407221794128418,\n",
       "  5.395112991333008,\n",
       "  5.378121376037598,\n",
       "  5.379026889801025,\n",
       "  5.354272842407227,\n",
       "  5.34442663192749,\n",
       "  5.323675632476807,\n",
       "  5.309948444366455,\n",
       "  5.295717716217041,\n",
       "  5.2823805809021,\n",
       "  5.278701305389404,\n",
       "  5.24625825881958,\n",
       "  5.238426685333252,\n",
       "  5.235356330871582,\n",
       "  5.208381652832031,\n",
       "  5.206161022186279,\n",
       "  5.17386531829834,\n",
       "  5.169784069061279,\n",
       "  5.14191198348999,\n",
       "  5.151164531707764,\n",
       "  5.118951320648193,\n",
       "  5.11637020111084,\n",
       "  5.105443477630615,\n",
       "  5.071829795837402,\n",
       "  5.0555901527404785,\n",
       "  5.059866905212402,\n",
       "  5.042524814605713,\n",
       "  5.024756908416748,\n",
       "  5.008509159088135,\n",
       "  5.006677150726318,\n",
       "  4.972083568572998,\n",
       "  4.957860469818115,\n",
       "  4.94718074798584,\n",
       "  4.937748432159424,\n",
       "  4.920748710632324,\n",
       "  4.900698184967041,\n",
       "  4.87948751449585,\n",
       "  4.863826274871826,\n",
       "  4.8704142570495605,\n",
       "  4.838154315948486,\n",
       "  4.834593296051025,\n",
       "  4.802982330322266,\n",
       "  4.797344207763672,\n",
       "  4.787419319152832,\n",
       "  4.771200656890869,\n",
       "  4.7608160972595215,\n",
       "  4.735986709594727,\n",
       "  4.724730968475342,\n",
       "  4.721199035644531,\n",
       "  4.699984073638916,\n",
       "  4.679603099822998,\n",
       "  4.661235809326172,\n",
       "  4.638885974884033,\n",
       "  4.626583099365234,\n",
       "  4.623318195343018,\n",
       "  4.618004322052002,\n",
       "  4.592109680175781,\n",
       "  4.572691917419434,\n",
       "  4.579319000244141,\n",
       "  4.542052268981934,\n",
       "  4.513645172119141,\n",
       "  4.526481628417969,\n",
       "  4.517587661743164,\n",
       "  4.469254016876221,\n",
       "  4.479573726654053,\n",
       "  4.468900203704834,\n",
       "  4.430369853973389,\n",
       "  4.427703380584717,\n",
       "  4.411497116088867,\n",
       "  4.40672492980957,\n",
       "  4.389951229095459,\n",
       "  4.371406555175781,\n",
       "  4.361611366271973,\n",
       "  4.3443474769592285,\n",
       "  4.322752475738525,\n",
       "  4.308882236480713,\n",
       "  4.28932523727417,\n",
       "  4.277020454406738,\n",
       "  4.26450252532959,\n",
       "  4.256409168243408,\n",
       "  4.236536502838135,\n",
       "  4.216099739074707,\n",
       "  4.21427059173584,\n",
       "  4.203006267547607,\n",
       "  4.173655986785889,\n",
       "  4.174441814422607,\n",
       "  4.152297019958496,\n",
       "  4.148728847503662,\n",
       "  4.117303371429443,\n",
       "  4.110634803771973,\n",
       "  4.096402168273926,\n",
       "  4.0809855461120605,\n",
       "  4.060425281524658,\n",
       "  4.059698581695557,\n",
       "  4.050370216369629,\n",
       "  4.036579132080078,\n",
       "  3.9982805252075195,\n",
       "  3.999985456466675,\n",
       "  3.987234354019165,\n",
       "  3.9668617248535156,\n",
       "  3.9501678943634033,\n",
       "  3.936966896057129,\n",
       "  3.9314703941345215,\n",
       "  3.9127039909362793,\n",
       "  3.901527166366577,\n",
       "  3.8645825386047363,\n",
       "  3.8640401363372803,\n",
       "  3.854064702987671,\n",
       "  3.8492205142974854,\n",
       "  3.8378515243530273,\n",
       "  3.8342158794403076,\n",
       "  3.8128662109375,\n",
       "  3.796729803085327,\n",
       "  3.7753853797912598,\n",
       "  3.7590017318725586,\n",
       "  3.748854637145996,\n",
       "  3.736220359802246,\n",
       "  3.7253270149230957,\n",
       "  3.722557306289673,\n",
       "  3.7050015926361084,\n",
       "  3.6865482330322266,\n",
       "  3.678697109222412,\n",
       "  3.654285430908203,\n",
       "  3.6565561294555664,\n",
       "  3.645765542984009,\n",
       "  3.6286165714263916,\n",
       "  3.6257007122039795,\n",
       "  3.605717182159424,\n",
       "  3.5835719108581543,\n",
       "  3.5610415935516357,\n",
       "  3.5641605854034424,\n",
       "  3.5569803714752197,\n",
       "  3.5465312004089355,\n",
       "  3.523192882537842,\n",
       "  3.5137851238250732,\n",
       "  3.5066943168640137,\n",
       "  3.4922025203704834,\n",
       "  3.497016668319702,\n",
       "  3.4685781002044678,\n",
       "  3.451462984085083,\n",
       "  3.4642462730407715,\n",
       "  3.43557071685791,\n",
       "  3.427417039871216,\n",
       "  3.407057285308838,\n",
       "  3.4198179244995117,\n",
       "  3.392428159713745,\n",
       "  3.39213228225708,\n",
       "  3.3797707557678223,\n",
       "  3.358783483505249,\n",
       "  3.3467679023742676,\n",
       "  3.3481693267822266,\n",
       "  3.3376829624176025,\n",
       "  3.3207032680511475,\n",
       "  3.3173720836639404,\n",
       "  3.303562879562378,\n",
       "  3.2832069396972656,\n",
       "  3.2837557792663574,\n",
       "  3.2729640007019043,\n",
       "  3.253384590148926,\n",
       "  3.243380546569824,\n",
       "  3.248016357421875,\n",
       "  3.245523452758789,\n",
       "  3.2267568111419678,\n",
       "  3.2150938510894775,\n",
       "  3.2098374366760254,\n",
       "  3.1967546939849854,\n",
       "  3.1983959674835205,\n",
       "  3.1741952896118164,\n",
       "  3.1702589988708496,\n",
       "  3.168018341064453,\n",
       "  3.1635451316833496,\n",
       "  3.136061429977417,\n",
       "  3.130297899246216,\n",
       "  3.1177966594696045,\n",
       "  3.1203248500823975,\n",
       "  3.124911069869995,\n",
       "  3.1022772789001465,\n",
       "  3.0929956436157227,\n",
       "  3.091923236846924,\n",
       "  3.0797812938690186,\n",
       "  3.0669195652008057,\n",
       "  3.0673508644104004,\n",
       "  3.0615317821502686,\n",
       "  3.044806480407715,\n",
       "  3.0400285720825195,\n",
       "  3.0303711891174316,\n",
       "  3.0151031017303467,\n",
       "  3.014065980911255,\n",
       "  3.0321240425109863,\n",
       "  3.002615451812744,\n",
       "  2.995591640472412,\n",
       "  2.9941635131835938,\n",
       "  2.9910335540771484,\n",
       "  2.9708852767944336,\n",
       "  2.9675848484039307,\n",
       "  2.9641199111938477,\n",
       "  2.956922769546509,\n",
       "  2.9602365493774414,\n",
       "  2.9400203227996826,\n",
       "  2.9484610557556152,\n",
       "  2.9262192249298096,\n",
       "  2.9230599403381348,\n",
       "  2.9220008850097656,\n",
       "  2.917262077331543,\n",
       "  2.9071712493896484,\n",
       "  2.896575450897217,\n",
       "  2.89335298538208],\n",
       " 'validation_accuracy': [0.0,\n",
       "  10.280000000000001,\n",
       "  10.280000000000001,\n",
       "  10.280000000000001,\n",
       "  15.559999999999999,\n",
       "  11.35,\n",
       "  11.35,\n",
       "  11.35,\n",
       "  11.35,\n",
       "  11.35]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
